<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-FSSR6942QL"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-FSSR6942QL');
    </script>

    <title>Explore until Confident: Efficient Exploration for Embodied Question Answering</title>
    <link rel="icon" type="image/png" href="img/favicon_2.png">

    <meta name="description" content="Explore until Confident: Efficient Exploration for Embodied Question Answering">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/styles/default.min.css">
    <link rel="stylesheet" href="css/app.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>

    <script src="js/app.js"></script>
	
    <style>
        .nav-pills {
          position: relative;
          display: inline;
        }
        .imtip {
          position: absolute;
          top: 0;
          left: 0;
        }
    </style>
</head>
<body>
    <div class="container" id="main">
        <div class="row mt-4">
            <h2 class="col-md-12 text-center">
                <b><font size="+5">Explore until Confident</font></b>: </br> Efficient Exploration for Embodied Question Answering </br>
            </h2>
        </div>

        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                <!-- <br> -->
                <li><a href="//allenzren.github.io">Allen Z. Ren</a></li>
                <li><a href="//jadenvc.github.io">Jaden Clark</a></li>
                <li><a href="//www.anushridixit.com">Anushri Dixit</a></li>
                <li><a href="//mashaitkina.weebly.com">Masha Itkina</a></li>
                <!-- <br> -->
                <li><a href="//irom-lab.princeton.edu/majumdar">Anirudha Majumdar</a></li>
                <li><a href="//dorsa.fyi">Dorsa Sadigh</a></li>
                <!-- <br> -->
                <br>
                    <a href="https://www.princeton.edu/">
                        <image src="img/PU1line.svg" height="40px"> 
                        <!-- Princeton University -->
                    </a>
                    &nbsp&nbsp&nbsp
                    <a href="https://www.stanford.edu/">
                        <image src="img/stanford.png" height="65px"> 
                        <!-- Stanford University -->
                    </a>
                    &nbsp&nbsp&nbsp
                    <a href="https://www.tri.global/">
                        <image src="img/toyota-research-institute.png" height="50px"> 
                        <!-- Stanford University -->
                    </a>
                </ul>
            </div>
            <!-- <div class="col-md-12 text-center">
                <h4>CoRL 2023, Best Student Paper</h4>
            </div> -->
        </div>

        <div class="row justify-content-md-center">
            <div class="col-md-2 text-center">
                <a href="https://arxiv.org/abs/">
                <image src="img/paper_small.png" height="45px">
                <h4><strong>Paper</strong></h4>
                <!-- <h6>Paper</h6> -->
                </a>
            </div>
            <div class="col-md-2 text-center">
                <a href="https://youtu.be/">
                <image src="img/youtube_icon.png" height="45px">
                <h4><strong>Video</strong></h4>
                </a>
            </div>
            <div class="col-md-2 text-center">
                <a href="https://github.com/">
                <image src="img/github.png" height="45px">
                <h4><strong>Code</strong></h4>
                </a>
            </div>
            <div class="col-md-2 text-center">
                <a href="https://github.com/">
                <image src="img/github.png" height="45px">
                <h4><strong>Dataset</strong></h4>
                </a>
            </div>
        </div>

        <!-- <div class="row mt-4">
            <div class="col-md-12 text-center">
                <video id="v0" width="70%" preload="metadata" playsinline muted loop autoplay>
                    <source src="videos/knowno-microwave-score-handbrake.mp4" type="video/mp4">
                </video>
            </div>
        </div> -->
        <!-- <div class="row mt-4">
            <div class="col-md-6 text-center">
                <video id="v0" width="100%" preload="metadata" playsinline muted loop autoplay>
                    <source src="videos/visual-prompting-hb.mp4" type="video/mp4">
                </video>
            </div>
            <div class="col-md-6 text-center">
                <video id="v1" width="100%" preload="metadata" playsinline muted loop autoplay>
                    <source src="videos/cp-hb.mp4" type="video/mp4">
                </video>
            </div>
        </div> -->
        <style>
            .video-container {
                position: relative;
                padding-bottom: 56.25%; /* 16:9 aspect ratio */
                height: 0;
                overflow: hidden;
            }
            .video-container video {
                position: absolute;
                top: 0;
                left: 0;
                width: 100%;
                height: 100%;
            }
        </style>
        <div class="row mt-4">
            <div class="col-md-6 text-center">
                <div class="video-container">
                    <video id="v0" preload="metadata" playsinline muted loop autoplay>
                        <source src="videos/sim-hb.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
            <div class="col-md-6 text-center">
                <div class="video-container">
                    <video id="v1" preload="metadata" playsinline muted loop autoplay>
                        <source src="videos/real-hb.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
        </div>
        
        <div class="row justify-content-md-center">
            <div class="col-md-12 col-lg-10">
                <h3 class="mt-4 mb-2">
                    Abstract
                </h3>
                <p class="text-justify">
                    We consider the problem of Embodied Question Answering (EQA), which refers to settings where an embodied agent such as a robot needs to actively explore an environment to gather information until it is confident about the answer to a question. In this work, we leverage the strong semantic reasoning capabilities of large vision-language models (VLMs) to efficiently explore and answer such questions. However, there are two main challenges when using VLMs in EQA: they do not have an internal memory for mapping the scene to be able to plan how to explore over time, and their confidence can be miscalibrated and can cause the robot to prematurely stop exploration or over-explore. We propose a method that first builds a semantic map of the scene based on depth information and via visual prompting of a VLM — leveraging its vast knowledge of relevant regions of the scene for exploration. Next, we use conformal prediction to calibrate the VLM's question answering confidence, allowing the robot to know when to stop exploration — leading to a more calibrated and efficient exploration strategy. To test our framework in simulation, we also contribute a new EQA dataset with diverse, realistic human-robot scenarios and scenes built upon the Habitat-Matterport 3D Research Dataset (HM3D). Both simulated and real robot experiments show our proposed approach improves the performance and efficiency over baselines that do no leverage VLM for exploration or do not calibrate its confidence.
                </p>
                <!-- <p style="text-align:center;">
                    <image src="img/trial.png" width="100%">
                </p> -->
            </div>
        </div>

        <div class="row justify-content-md-center">
            <!-- <br> -->
            <div class="col-md-6 text-center">
                <iframe width="560" height="315" src="https://www.youtube.com/embed/xCXx09gfhx4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
            </div>
            <!-- <div class="col-md-6 text-center">
                <blockquote class="twitter-tweet"><p lang="en" dir="ltr" width="250">LLMs can generate plans and write robot code 📝 but they can also make mistakes. How do we get LLMs to 𝘬𝘯𝘰𝘸 𝘸𝘩𝘦𝘯 𝘵𝘩𝘦𝘺 𝘥𝘰𝘯&#39;𝘵 𝘬𝘯𝘰𝘸 🤷 and ask for help?<br><br>Read more on how we can do this (with statistical guarantees) for LLMs on robots 👇<a href="https://t.co/D7mHGzNP3p">https://t.co/D7mHGzNP3p</a> <a href="https://t.co/M9lUqlZ5cB">pic.twitter.com/M9lUqlZ5cB</a></p>&mdash; Allen Z. Ren (@allenzren) <a href="https://twitter.com/allenzren/status/1677000811803443213?ref_src=twsrc%5Etfw">July 6, 2023</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
            </div> -->
        </div>

        <!-- <div class="row justify-content-md-center">
            <blockquote class="twitter-tweet"><p lang="en" dir="ltr">LLMs can generate plans and write robot code 📝 but they can also make mistakes. How do we get LLMs to 𝘬𝘯𝘰𝘸 𝘸𝘩𝘦𝘯 𝘵𝘩𝘦𝘺 𝘥𝘰𝘯&#39;𝘵 𝘬𝘯𝘰𝘸 🤷 and ask for help?<br><br>Read more on how we can do this (with statistical guarantees) for LLMs on robots 👇<a href="https://t.co/D7mHGzNP3p">https://t.co/D7mHGzNP3p</a> <a href="https://t.co/M9lUqlZ5cB">pic.twitter.com/M9lUqlZ5cB</a></p>&mdash; Allen Z. Ren (@allenzren) <a href="https://twitter.com/allenzren/status/1677000811803443213?ref_src=twsrc%5Etfw">July 6, 2023</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
        </div> -->

        <div class="row justify-content-md-center">
            <div class="col-md-12 col-lg-10">
                <br>
                <h3>
                    Embodied Question Answering (EQA)
                </h3>
                <p class="text-justify">
                    In EQA tasks, the robot starts at a random location in a 3D scene, explores the space,and stops when it is confident about answering the question. This can be a challenging problem due to highly diverse scenes and lack of an a-priori map of the environment. Previous works rely on training dedicated exploration policies and question answering modules from scratch, which can be data-inefficient and only handle simple questions.
                    <br><p style="text-align:center;">
                        <image src="img/trial.png" width="60%">
                    </p>
                    We are interested in using pre-trained large <strong>vision-language models (VLMs)</strong> for EQA without additional training. VLMs have achieved impressive performance in answering complex questions about static 2D images that sometimes requires reasoning, and we find VLMs can also reason about semantically relevant regions to explore given the question and view. However, there are still two main challenges:<br>
                    <strong>1) Limited Internal Memory of VLMs.</strong> EQA benefits from the robot tracking previously explored regions and also ones yet to be explored but relevant for answering the question. However, VLMs do not have an internal memory for mapping the scene and storing such semantic information;<br>
                    <strong>2) Miscalibrated VLMs.</strong> VLMs are fine-tuned on pre-trained large language models (LLMs) as the language decoder, and LLMs are often miscalibrated - that is they can be over-confident or under-confident about the output. This makes it difficult to determine when the robot is confident enough about question answering in EQA and then stop exploration.
            </div>
            <br>
        </div>

        <div class="row justify-content-md-center">
            <div class="col-md-12 col-lg-10">
                <h3>
                    How can we endow VLMs with the capability of efficient exploration for EQA?
                </h3>
                <p class="text-justify">
                    Addressing the first challenge of limited internal memory, we propose building a map of the scene external to the VLM as the robot visits different locations. On top of it, we embed the VLM's knowledge about possible exploration directions into this <strong>semantic map</strong> to guide the robot's exploration. Such semantic information is obtained by <strong>visual prompting</strong>: annotating the free space in the current image view, prompting the VLM to choose among the unoccupied regions, and querying its prediction. The values are then stored in the semantic map.
                <!-- <br><br><p style="text-align:center;">
                    <image src="img/mcqa-cp.png" width="100%">
                </p><br> -->
                <div class="row mt-4">
                    <div class="col-md-12 text-center">
                        <video id="v0" width="70%" preload="metadata" playsinline muted loop autoplay>
                            <source src="videos/visual-prompting-hb.mp4" type="video/mp4">
                        </video>
                    </div>
                </div>
                We then leverage such semantic information with Frontier-Based Exploration (FBE). Frontiers are the locations between explored and unexplored regions, and we apply weighted sampling of the frontiers based on their semantic values on the map.
                <br><br><p style="text-align:center;">
                    <image src="img/semantic-map.png" width="60%">
                </p>
                <!-- <br>
                <div class="row mt-4">
                    <div class="col-md-12 text-center">
                        <video id="v0" width="100%" preload="metadata" playsinline muted loop autoplay>
                            <source src="videos/knowno-cp-handbrake.mp4" type="video/mp4">
                        </video>
                    </div>
                </div>
                <br> -->
                Addressing the second issue of miscalibration, we leverage <strong>multi-step conformal prediction</strong>, which allows the robot to maintain a set of possible answers (<strong>prediction set</strong>) over time, and stop when the set reduces to a single answer. Conformal prediction uses a moderately sized (e.g., ~300) set of scenarios for carefully selecting a confidence threshold above which answers are included in the prediction set. This procedure allows us to achieve <strong>calibrated confidence</strong>: with a user-specified probability, the prediction set is guaranteed to contain the correct answer for a new scenario (under the assumption that calibration and test scenarios are drawn from the same unknown distribution). CP minimizes the prediction set size, which helps the robot to stop as quickly as it can while satisfying calibrated confidence.
                <div class="row mt-4">
                    <div class="col-md-12 text-center">
                        <video id="v0" width="70%" preload="metadata" playsinline muted loop autoplay>
                            <source src="videos/cp-hb.mp4" type="video/mp4">
                        </video>
                    </div>
                </div>
            </div>
        </div>

        <div class="row justify-content-md-center mt-4">
            <div class="col-md-10 col-lg-8">
                <h3>Experiment Videos</h3>
                <div class="card mb-4 mt-4 text-dark bg-light">
                    <h5 class="card-header">Simulated Scenarios in Habitat-Sim</h5>
                    <div class="card-body">

                        <p>Choose a user instruction:</p>
                        <select class="form-select" size="8" aria-label="size 5 select example">
                            <option value="saycan_1">
                                1) Pick up the bottled water.
                            </option>
                            <option value="saycan_2">
                                2) Bring me the chips.
                            </option>
                            <option value="saycan_3">
                                3) Put the apple in the drawer.
                            </option>
                            <option value="saycan_4">
                                4) Put the rice chips in the top drawer.
                            </option>
                            <option value="saycan_5">
                                5) Put the chips in the drawer.
                            </option>
                            <option value="saycan_6">
                                6) Put the soda in the drawer.
                            </option>
                            <option value="saycan_7">
                                7) Put the apple near the bottle.
                            </option>
                            <option value="saycan_8">
                                8) Put the multigrain chips near the fruit.
                            </option>
                            <option value="saycan_9">
                                9) There is the kettle chips and RedBull. I ate it already. Can you dispose of it?
                            </option>
                            <option value="saycan_10">
                                10) There is the sponge and energy bar. I don't want to use it for clearning any more. Can you please dispose of it?
                            </option>
                            <option value="saycan_11">
                                11) Can you dispose of that fruit?
                            </option>
                            <option value="saycan_12">
                                12) There is the bottled tea and Coke. It is too sweet. Can you dispose of it?
                            </option>
                            <option value="saycan_13">
                                13) Put the bowl in the microwave.
                            </option>
                            <option value="saycan_14">
                                14) Put the bowl on the stove.
                            <option value="saycan_15">
                                15) Can you dispose of the chips? It should have expired.
                            </option>
                            <option value="saycan_16">
                                16) Put the RedBull in the top drawer.
                            </option>                            
                        </select>

                        <div class="col-md-12 mb-2">
                            <video id="vid_saycan" width="100%" preload="metadata" playsinline controls>
                                <source src="videos/saycan/saycan_all_480p.mp4" type="video/mp4">
                            </video>
                        </div>

                        <div class="row" id="content_saycan_1">
                            <div class="col-md-12" id="code_saycan_1">
                            </div>
                        </div>
                        <div class="row" id="content_saycan_2" style="display:none">
                            <div class="col-md-12" id="code_saycan_2">
                            </div>
                        </div>
                        <div class="row" id="content_saycan_3" style="display:none">
                            <div class="col-md-12" id="code_saycan_3">
                            </div>
                        </div>
                        <div class="row" id="content_saycan_4" style="display:none">
                            <div class="col-md-12" id="code_saycan_4">
                            </div>
                        </div>
                        <div class="row" id="content_saycan_5" style="display:none">
                            <div class="col-md-12" id="code_saycan_5">
                            </div>
                        </div>
                        <div class="row" id="content_saycan_6" style="display:none">
                            <div class="col-md-12" id="code_saycan_6">
                            </div>
                        </div>
                        <div class="row" id="content_saycan_7" style="display:none">
                            <div class="col-md-12" id="code_saycan_7">
                            </div>
                        </div>
                        <div class="row" id="content_saycan_8" style="display:none">
                            <div class="col-md-12" id="code_saycan_8">
                            </div>
                        </div>
                        <div class="row" id="content_saycan_9" style="display:none">
                            <div class="col-md-12" id="code_saycan_9">
                            </div>
                        </div>
                        <div class="row" id="content_saycan_10" style="display:none">
                            <div class="col-md-12" id="code_saycan_10">
                            </div>
                        </div>
                        <div class="row" id="content_saycan_11" style="display:none">
                            <div class="col-md-12" id="code_saycan_11">
                            </div>
                        </div>
                        <div class="row" id="content_saycan_12" style="display:none">
                            <div class="col-md-12" id="code_saycan_12">
                            </div>
                        </div>
                        <div class="row" id="content_saycan_13" style="display:none">
                            <div class="col-md-12" id="code_saycan_13">
                            </div>
                        </div>
                        <div class="row" id="content_saycan_14" style="display:none">
                            <div class="col-md-12" id="code_saycan_14">
                            </div>
                        </div>
                        <div class="row" id="content_saycan_15" style="display:none">
                            <div class="col-md-12" id="code_saycan_15">
                            </div>
                        </div>
                        <div class="row" id="content_saycan_16" style="display:none">
                            <div class="col-md-12" id="code_saycan_16">
                            </div>
                        </div>
                    </div>
                </div>

                <div class="card mb-4 mt-4 text-dark bg-light">
                    <h5 class="card-header">Real Scenarios with Fetch Robot</h5>
                    <div class="card-body">

                        <p>User instruction: Sort the items that human likes in the blue plate, and ones human dislikes in the green plate.<br>Items involved in each trial:</p>
                        <select class="form-select" size="8" aria-label="size 5 select example">
                            <option value="ur5_1">
                                1) sunny-side-up egg, lettuce, mustard
                            </option>
                            <option value="ur5_2">
                                2) mango, peanut butter, carrot
                            </option>
                            <option value="ur5_3">
                                3) orange, pretzel, peach
                            </option>
                            <option value="ur5_4">
                                4) carrot, pizza slice, egg
                            </option>
                            <option value="ur5_5">
                                5) Skittles, lemon, pea
                            </option>
                            <option value="ur5_6">
                                6) sunny-side-up egg, chicken drumstick, mustard
                            </option>
                            <option value="ur5_7">
                                7) donut, carrot, egg
                            </option>
                            <option value="ur5_8">
                                8) donut, waffle, pear
                            </option>
                            <option value="ur5_9">
                                9) tomato, peach, apple
                            </option>
                            <option value="ur5_10">
                                10) egg, Skittles, lettuce
                            </option>
                        </select>

                        <div class="col-md-12 mb-2">
                            <video id="vid_ur5" width="100%" preload="metadata" playsinline controls>
                                <source src="videos/ur5/ur5_all_handbrake_crop_720p.mp4" type="video/mp4">
                            </video>
                        </div>

                        <div class="row" id="content_ur5_1">
                            <div class="col-md-12" id="code_ur5_1">
                            </div>
                        </div>
                        <div class="row" id="content_ur5_2" style="display:none">
                            <div class="col-md-12" id="code_ur5_2">
                            </div>
                        </div>
                        <div class="row" id="content_ur5_3" style="display:none">
                            <div class="col-md-12" id="code_ur5_3">
                            </div>
                        </div>
                        <div class="row" id="content_ur5_4" style="display:none">
                            <div class="col-md-12" id="code_ur5_4">
                            </div>
                        </div>
                        <div class="row" id="content_ur5_5" style="display:none">
                            <div class="col-md-12" id="code_ur5_5">
                            </div>
                        </div>
                        <div class="row" id="content_ur5_6" style="display:none">
                            <div class="col-md-12" id="code_ur5_6">
                            </div>
                        </div>
                        <div class="row" id="content_ur5_7" style="display:none">
                            <div class="col-md-12" id="code_ur5_7">
                            </div>
                        </div>
                        <div class="row" id="content_ur5_8" style="display:none">
                            <div class="col-md-12" id="code_ur5_8">
                            </div>
                        </div>
                        <div class="row" id="content_ur5_9" style="display:none">
                            <div class="col-md-12" id="code_ur5_9">
                            </div>
                        </div>
                        <div class="row" id="content_ur5_10" style="display:none">
                            <div class="col-md-12" id="code_ur5_10">
                            </div>
                        </div>
                    </div>
                </div>

            </div>
        </div>
        <div class="row justify-content-md-center">
            <div class="col-md-12 col-lg-10">
                <h3>
                    Citation 
                </h3>
                <a href="https://arxiv.org/abs/2307.01928">[arxiv version]</a>
                <div class="form-group col-md-12">
                    <textarea id="bibtex" class="form-control" rows="5" readonly>
@inproceedings{exploreeqa2024,
    title={Explore until Confident: Efficient Exploration for Embodied Question Answering},
    author={Ren, Allen Z. and Clark, Jaden and Dixit, Anushri and Itkina, Masha and Majumdar, Anirudha and Sadigh, Dorsa},
    booktitle={arXiv preprint arXiv:2307.01928},
    year={2024}
}</textarea>
                </div>
            </div>
        </div>
        <div class="row justify-content-md-center mt-4">
            <div class="col-md-12 col-lg-10">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                    This work was partially supported by the NSF CAREER Award [\#2044149] and the Office of Naval Research [N00014-23-1-2148]. The website template is from <a href="https://robot-help.github.io/">KnowNo</a>.
                </p>
            </div>
        </div>
    </div>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM" crossorigin="anonymous"></script>
    <script>hljs.highlightAll();</script>
</body>
</html>
